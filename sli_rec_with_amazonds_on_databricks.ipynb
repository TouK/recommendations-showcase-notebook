{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3cf9cc-51fa-454c-bcd9-30659cc30175",
   "metadata": {},
   "source": [
    "## Step 0 - import model libraries\n",
    "\n",
    "The example is based on the Sli_Rec model from the [recommenders](https://github.com/recommenders-team/recommenders) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d1ba22b-5aaa-41c8-865d-9d5d18419043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.20 (main, Oct  3 2024, 07:27:41) \n",
      "[GCC 11.2.0]\n",
      "Tensorflow version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n",
    "from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
    "from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n",
    "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "\n",
    "RANDOM_SEED = SEED\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Tensorflow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf661c4-8df2-41d9-b0bb-0bc676d2ec53",
   "metadata": {},
   "source": [
    "## Step 0 - model training\n",
    "\n",
    "Model is trained on the Amazon dataset. The general flow of the training process is based on the [Sequential Recommender Quick Start](https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5276c64d-7139-4eaf-945d-869818a60a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 692k/692k [09:45<00:00, 1.18kKB/s]\n",
      "100%|███████████████████████████████████████| 97.5k/97.5k [01:46<00:00, 914KB/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(\"training\", \"resources\")\n",
    "\n",
    "train_file = os.path.join(data_path, r'train_data')\n",
    "valid_file = os.path.join(data_path, r'valid_data')\n",
    "test_file = os.path.join(data_path, r'test_data')\n",
    "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
    "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
    "cate_vocab = os.path.join(data_path, r'category_vocab.pkl')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "reviews_name = 'reviews_Movies_and_TV_5.json'\n",
    "meta_name = 'meta_Movies_and_TV.json'\n",
    "reviews_file = os.path.join(data_path, reviews_name)\n",
    "meta_file = os.path.join(data_path, meta_name)\n",
    "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
    "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
    "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
    "sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
    "\n",
    "input_files = [reviews_file, meta_file, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    download_and_extract(reviews_name, reviews_file)\n",
    "    download_and_extract(meta_name, meta_file)\n",
    "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba0123-265a-47f5-8ab4-edbbfe8f53b1",
   "metadata": {},
   "source": [
    "Prepare model hyperparameters.\n",
    "\n",
    "Note: remember to use `_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)` to generate the user_vocab, item_vocab and cate_vocab files, if you are using your own dataset rather than using our demo Amazon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575f816f-a152-4b88-a47c-a6a606cb7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 400\n",
    "yaml_train_config_file = 'training/model_train_config.yaml'\n",
    "\n",
    "hparams = prepare_hparams(yaml_train_config_file, \n",
    "                          embed_l2=0., \n",
    "                          layer_l2=0.,\n",
    "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          show_step=20,\n",
    "                          MODEL_DIR=os.path.join(data_path, \"training/model\"),\n",
    "                          SUMMARIES_DIR=os.path.join(data_path, \"training/summary/\"),\n",
    "                          user_vocab=user_vocab,\n",
    "                          item_vocab=item_vocab,\n",
    "                          cate_vocab=cate_vocab,\n",
    "                          need_sample=True,\n",
    "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8240b83-b4d0-441e-a29e-3b28c5d41cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = SequentialIterator\n",
    "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c913f78-dfe0-4cf0-a517-18a077ff8eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 1.6104, data_loss: 1.6104\n",
      "step 40 , total_loss: 1.6080, data_loss: 1.6080\n",
      "eval valid at epoch 1: auc:0.5029,logloss:0.693,mean_mrr:0.4611,ndcg@2:0.3336,ndcg@4:0.5193,ndcg@6:0.5932,group_auc:0.5089\n",
      "step 20 , total_loss: 1.5448, data_loss: 1.5448\n",
      "step 40 , total_loss: 1.4160, data_loss: 1.4160\n",
      "eval valid at epoch 2: auc:0.6295,logloss:0.7074,mean_mrr:0.5561,ndcg@2:0.4683,ndcg@4:0.6286,ndcg@6:0.6663,group_auc:0.6332\n",
      "step 20 , total_loss: 1.3607, data_loss: 1.3607\n",
      "step 40 , total_loss: 1.3485, data_loss: 1.3485\n",
      "eval valid at epoch 3: auc:0.6723,logloss:0.805,mean_mrr:0.5867,ndcg@2:0.5105,ndcg@4:0.6574,ndcg@6:0.6896,group_auc:0.6656\n",
      "step 20 , total_loss: 1.3178, data_loss: 1.3178\n",
      "step 40 , total_loss: 1.2463, data_loss: 1.2463\n",
      "eval valid at epoch 4: auc:0.6976,logloss:0.6739,mean_mrr:0.6195,ndcg@2:0.5549,ndcg@4:0.6848,ndcg@6:0.7143,group_auc:0.6934\n",
      "step 20 , total_loss: 1.2371, data_loss: 1.2371\n",
      "step 40 , total_loss: 1.2622, data_loss: 1.2622\n",
      "eval valid at epoch 5: auc:0.7233,logloss:0.6679,mean_mrr:0.6467,ndcg@2:0.5875,ndcg@4:0.7082,ndcg@6:0.7348,group_auc:0.7174\n",
      "step 20 , total_loss: 1.2438, data_loss: 1.2438\n",
      "step 40 , total_loss: 1.2483, data_loss: 1.2483\n",
      "eval valid at epoch 6: auc:0.7327,logloss:0.6479,mean_mrr:0.6523,ndcg@2:0.5937,ndcg@4:0.7127,ndcg@6:0.739,group_auc:0.7215\n",
      "step 20 , total_loss: 1.2177, data_loss: 1.2177\n",
      "step 40 , total_loss: 1.2320, data_loss: 1.2320\n",
      "eval valid at epoch 7: auc:0.7341,logloss:0.6018,mean_mrr:0.6574,ndcg@2:0.5993,ndcg@4:0.7167,ndcg@6:0.7428,group_auc:0.7257\n",
      "step 20 , total_loss: 1.2238, data_loss: 1.2238\n",
      "step 40 , total_loss: 1.1878, data_loss: 1.1878\n",
      "eval valid at epoch 8: auc:0.7381,logloss:0.6337,mean_mrr:0.6635,ndcg@2:0.6094,ndcg@4:0.7221,ndcg@6:0.7475,group_auc:0.733\n",
      "step 20 , total_loss: 1.1570, data_loss: 1.1570\n",
      "step 40 , total_loss: 1.1572, data_loss: 1.1572\n",
      "eval valid at epoch 9: auc:0.7346,logloss:0.6586,mean_mrr:0.6594,ndcg@2:0.6028,ndcg@4:0.7177,ndcg@6:0.7444,group_auc:0.7277\n",
      "step 20 , total_loss: 1.2016, data_loss: 1.2016\n",
      "step 40 , total_loss: 1.1366, data_loss: 1.1366\n",
      "eval valid at epoch 10: auc:0.7429,logloss:0.6031,mean_mrr:0.671,ndcg@2:0.6154,ndcg@4:0.7282,ndcg@6:0.753,group_auc:0.7375\n",
      "[(1, {'auc': 0.5029, 'logloss': 0.693, 'mean_mrr': 0.4611, 'ndcg@2': 0.3336, 'ndcg@4': 0.5193, 'ndcg@6': 0.5932, 'group_auc': 0.5089}), (2, {'auc': 0.6295, 'logloss': 0.7074, 'mean_mrr': 0.5561, 'ndcg@2': 0.4683, 'ndcg@4': 0.6286, 'ndcg@6': 0.6663, 'group_auc': 0.6332}), (3, {'auc': 0.6723, 'logloss': 0.805, 'mean_mrr': 0.5867, 'ndcg@2': 0.5105, 'ndcg@4': 0.6574, 'ndcg@6': 0.6896, 'group_auc': 0.6656}), (4, {'auc': 0.6976, 'logloss': 0.6739, 'mean_mrr': 0.6195, 'ndcg@2': 0.5549, 'ndcg@4': 0.6848, 'ndcg@6': 0.7143, 'group_auc': 0.6934}), (5, {'auc': 0.7233, 'logloss': 0.6679, 'mean_mrr': 0.6467, 'ndcg@2': 0.5875, 'ndcg@4': 0.7082, 'ndcg@6': 0.7348, 'group_auc': 0.7174}), (6, {'auc': 0.7327, 'logloss': 0.6479, 'mean_mrr': 0.6523, 'ndcg@2': 0.5937, 'ndcg@4': 0.7127, 'ndcg@6': 0.739, 'group_auc': 0.7215}), (7, {'auc': 0.7341, 'logloss': 0.6018, 'mean_mrr': 0.6574, 'ndcg@2': 0.5993, 'ndcg@4': 0.7167, 'ndcg@6': 0.7428, 'group_auc': 0.7257}), (8, {'auc': 0.7381, 'logloss': 0.6337, 'mean_mrr': 0.6635, 'ndcg@2': 0.6094, 'ndcg@4': 0.7221, 'ndcg@6': 0.7475, 'group_auc': 0.733}), (9, {'auc': 0.7346, 'logloss': 0.6586, 'mean_mrr': 0.6594, 'ndcg@2': 0.6028, 'ndcg@4': 0.7177, 'ndcg@6': 0.7444, 'group_auc': 0.7277}), (10, {'auc': 0.7429, 'logloss': 0.6031, 'mean_mrr': 0.671, 'ndcg@2': 0.6154, 'ndcg@4': 0.7282, 'ndcg@6': 0.753, 'group_auc': 0.7375})]\n",
      "best epoch: 10\n",
      "Time cost for training is 2.88 mins\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
    "\n",
    "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
    "# we will evaluate the performance of model on valid_file every epoch\n",
    "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a9acf1f-c98c-403e-a4dd-b471ef847797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.7166, 'logloss': 0.6136, 'mean_mrr': 0.489, 'ndcg@2': 0.4021, 'ndcg@4': 0.5018, 'ndcg@6': 0.5541, 'group_auc': 0.7065}\n"
     ]
    }
   ],
   "source": [
    "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d500e5-cc1d-4bf6-96a8-07e601a2bc62",
   "metadata": {},
   "source": [
    "## Step 2 - log model to Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f361d759-7c2e-40af-89c0-9e456ff354c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import mlflow\n",
    "import os\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.pyfunc import PythonModelContext\n",
    "from typing import Optional, Dict, Any\n",
    "from mlflow.types import Schema, ColSpec, TensorSpec\n",
    "from mlflow.models import ModelSignature\n",
    "\n",
    "print(mlflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a9afb-ec45-473d-a6fc-0fe9a415dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the experiment name in the Databricks specific format\n",
    "EXPERIMENT_NAME = \"/Users/<insert_your_username>/simple_rec_experiment\"\n",
    "DATABRICKS_CLIENT_ID = \"<insert_your_client_id>\"\n",
    "DATABRICKS_CLIENT_SECRET = \"<insert_your_client_secret>\"\n",
    "DATABRICKS_HOST = \"https://<insert_the_host_name>.azuredatabricks.net\"\n",
    "\n",
    "os.environ[\"DATABRICKS_HOST\"] = DATABRICKS_HOST\n",
    "os.environ[\"DATABRICKS_CLIENT_ID\"] = DATABRICKS_CLIENT_ID\n",
    "os.environ[\"DATABRICKS_CLIENT_SECRET\"] = DATABRICKS_CLIENT_SECRET\n",
    "mlflow.set_registry_uri(\"databricks\")\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e5839db-04b4-452c-a84f-59b6121703e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mst/anaconda3/envs/test_rec_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 3955.02it/s]\n",
      "Downloading artifacts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2187.95it/s]\n",
      "Downloading artifacts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4245.25it/s]\n",
      "Downloading artifacts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2468.69it/s]\n",
      "Downloading artifacts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2686.93it/s]\n",
      "Uploading artifacts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.58it/s]\n",
      "Registered model 'slirec_model' already exists. Creating a new version of this model...\n",
      "2024/10/24 17:25:31 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: slirec_model, version 6\n",
      "Created version '6' of model 'slirec_model'.\n"
     ]
    }
   ],
   "source": [
    "artifacts = {\n",
    "    \"model_data\" : \"training/resources/training/model/\",\n",
    "    \"model_config\": \"serving/model_serve_config.yaml\",\n",
    "    \"user_vocab\" : \"training/resources/user_vocab.pkl\",\n",
    "    \"item_vocab\" : \"training/resources/item_vocab.pkl\",\n",
    "    \"category_vocab\" : \"training/resources/category_vocab.pkl\",\n",
    "    \"item_category_dict\": \"training/resources/item_cat_dict.pkl\"\n",
    "}\n",
    "\n",
    "class SliRecModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def load_context(self, context):\n",
    "        from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n",
    "        from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "        from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n",
    "        import numpy as np\n",
    "\n",
    "        hparams = prepare_hparams(\n",
    "            context.artifacts[\"model_config\"],\n",
    "            user_vocab=context.artifacts[\"user_vocab\"],\n",
    "            item_vocab=context.artifacts[\"item_vocab\"],\n",
    "            cate_vocab=context.artifacts[\"category_vocab\"],\n",
    "        )\n",
    "        \n",
    "        self.model = SeqModel(hparams, SequentialIterator)\n",
    "        self.model.load_model(context.artifacts[\"model_data\"] + \"./artifacts/best_model\")\n",
    "        self.item_cat_dict = self.load_dict(context.artifacts[\"item_category_dict\"])\n",
    "        self.TOP_N_HIGHEST_RECOMMENDATIONS = 12\n",
    "        self.BATCH_SIZE = 16\n",
    "        \n",
    "    def predict(self, context, model_input):\n",
    "        user_id = model_input['userId'][0]\n",
    "        history_item_ids = model_input['itemIds'][0]\n",
    "        history_timestamps = model_input['timestamps'][0]\n",
    "\n",
    "        batches = self.build_batches_generator(user_id, history_item_ids, history_timestamps)\n",
    "\n",
    "        # TODO: confirm the type conversions and collection operations are optimal here\n",
    "        inferenced_items = []\n",
    "        inferenced_preds = []\n",
    "        for (batch, items) in batches:\n",
    "            preds = self.model.infer(self.model.sess, batch)\n",
    "            inferenced_preds += (preds[0].flatten().tolist())\n",
    "            inferenced_items += items\n",
    "\n",
    "        top_n_item_indices = np.argsort(inferenced_preds)[::-1][:self.TOP_N_HIGHEST_RECOMMENDATIONS]\n",
    "        return [{\n",
    "            \"items\": np.array(inferenced_items)[top_n_item_indices].tolist(),\n",
    "            \"preds\": np.array(inferenced_preds)[top_n_item_indices].tolist()\n",
    "        }]\n",
    "\n",
    "    def build_batches_generator(self, user_id, history_item_ids, history_timestamps, batch_num_ngs=0, min_seq_length=1):\n",
    "        import time\n",
    "\n",
    "        now_timestamp = int(time.time())\n",
    "        it = self.model.iterator\n",
    "        history_item_categories = [self.item_cat_dict[k] for k in history_item_ids]\n",
    "        \n",
    "        batched_item_ids = []\n",
    "        label_list = []\n",
    "        user_list = []\n",
    "        item_list = []\n",
    "        item_cate_list = []\n",
    "        item_history_batch = []\n",
    "        item_cate_history_batch = []\n",
    "        time_list = []\n",
    "        time_diff_list = []\n",
    "        time_from_first_action_list = []\n",
    "        time_to_now_list = []\n",
    "\n",
    "        cnt = 0\n",
    "        for item_id, item_category in self.item_cat_dict.items():\n",
    "            encoded_item = self.encode_single_item(\n",
    "                user_id,\n",
    "                item_id,\n",
    "                item_category,\n",
    "                now_timestamp,\n",
    "                history_item_ids,\n",
    "                history_item_categories,\n",
    "                history_timestamps\n",
    "            )\n",
    "\n",
    "            if len(encoded_item[\"historyItemIds\"]) < min_seq_length:\n",
    "                continue\n",
    "\n",
    "            batched_item_ids.append(item_id)\n",
    "            user_list.append(encoded_item[\"userId\"])\n",
    "            item_list.append(encoded_item[\"itemId\"])\n",
    "            item_cate_list.append(encoded_item[\"itemCategory\"])\n",
    "            item_history_batch.append(encoded_item[\"historyItemIds\"])\n",
    "            item_cate_history_batch.append(encoded_item[\"historyCategories\"])\n",
    "            time_list.append(encoded_item[\"currentTime\"])\n",
    "            time_diff_list.append(encoded_item[\"timeDiff\"])\n",
    "            time_from_first_action_list.append(encoded_item[\"timeFromFirstAction\"])\n",
    "            time_to_now_list.append(encoded_item[\"timeToNow\"])\n",
    "\n",
    "            # label is useless for prediction but required for SliRec conversion utilities\n",
    "            label_list.append(0)\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt == self.BATCH_SIZE:\n",
    "                res = it._convert_data(\n",
    "                    label_list,\n",
    "                    user_list,\n",
    "                    item_list,\n",
    "                    item_cate_list,\n",
    "                    item_history_batch,\n",
    "                    item_cate_history_batch,\n",
    "                    time_list,\n",
    "                    time_diff_list,\n",
    "                    time_from_first_action_list,\n",
    "                    time_to_now_list,\n",
    "                    batch_num_ngs,\n",
    "                )\n",
    "                batch_feed_dict = it.gen_feed_dict(res)\n",
    "                yield (batch_feed_dict, batched_item_ids) if batch_feed_dict else None\n",
    "                \n",
    "                batched_item_ids = []\n",
    "                label_list = []\n",
    "                user_list = []\n",
    "                item_list = []\n",
    "                item_cate_list = []\n",
    "                item_history_batch = []\n",
    "                item_cate_history_batch = []\n",
    "                time_list = []\n",
    "                time_diff_list = []\n",
    "                time_from_first_action_list = []\n",
    "                time_to_now_list = []\n",
    "                cnt = 0\n",
    "        # process the remaining inputs in the last batch\n",
    "        if cnt > 0:\n",
    "            res = it._convert_data(\n",
    "                label_list,\n",
    "                user_list,\n",
    "                item_list,\n",
    "                item_cate_list,\n",
    "                item_history_batch,\n",
    "                item_cate_history_batch,\n",
    "                time_list,\n",
    "                time_diff_list,\n",
    "                time_from_first_action_list,\n",
    "                time_to_now_list,\n",
    "                batch_num_ngs,\n",
    "            )\n",
    "            batch_feed_dict = it.gen_feed_dict(res)\n",
    "            yield (batch_feed_dict, batched_item_ids) if batch_feed_dict else None\n",
    "\n",
    "    # extracted and adjusted based on the SequentialIterator from the recommenders module\n",
    "    # https://github.com/recommenders-team/recommenders/blob/main/recommenders/models/deeprec/io/sequential_iterator.py\n",
    "    def encode_single_item(self, userId, itemId, itemCategory, nowTimestamp, historyItemIds, historyCategories, historyTimestamps):\n",
    "        it = self.model.iterator\n",
    "        \n",
    "        user_id = it.userdict[userId] if userId in it.userdict else 0\n",
    "        item_id = it.itemdict[itemId] if itemId in it.itemdict else 0\n",
    "        item_cate = it.catedict[itemCategory] if itemCategory in it.catedict else 0\n",
    "        current_time = float(nowTimestamp)\n",
    "\n",
    "        item_history_sequence = []\n",
    "        cate_history_sequence = []\n",
    "        time_history_sequence = []\n",
    "    \n",
    "        for item in historyItemIds:\n",
    "            item_history_sequence.append(\n",
    "                it.itemdict[item] if item in it.itemdict else 0\n",
    "            )\n",
    "        \n",
    "        for cate in historyCategories:\n",
    "            cate_history_sequence.append(\n",
    "                it.catedict[cate] if cate in it.catedict else 0\n",
    "            )\n",
    "\n",
    "        time_history_sequence = [float(i) for i in historyTimestamps]\n",
    "        time_range = 3600 * 24\n",
    "\n",
    "        time_diff = []\n",
    "        for i in range(len(time_history_sequence) - 1):\n",
    "            diff = (time_history_sequence[i + 1] - time_history_sequence[i]) / time_range\n",
    "            diff = max(diff, 0.5)\n",
    "            time_diff.append(diff)\n",
    "    \n",
    "        last_diff = (current_time - time_history_sequence[-1]) / time_range\n",
    "        last_diff = max(last_diff, 0.5)\n",
    "        time_diff.append(last_diff)\n",
    "        time_diff = np.log(time_diff)\n",
    "\n",
    "        time_from_first_action = []\n",
    "        first_time = time_history_sequence[0]\n",
    "        time_from_first_action = [\n",
    "            (t - first_time) / time_range for t in time_history_sequence[1:]\n",
    "        ]\n",
    "        time_from_first_action = [max(t, 0.5) for t in time_from_first_action]\n",
    "        last_diff = (current_time - first_time) / time_range\n",
    "        last_diff = max(last_diff, 0.5)\n",
    "        time_from_first_action.append(last_diff)\n",
    "        time_from_first_action = np.log(time_from_first_action)\n",
    "\n",
    "        time_to_now = []\n",
    "        time_to_now = [(current_time - t) / time_range for t in time_history_sequence]\n",
    "        time_to_now = [max(t, 0.5) for t in time_to_now]\n",
    "        time_to_now = np.log(time_to_now)\n",
    "\n",
    "        return {\n",
    "            \"userId\": user_id,\n",
    "            \"itemId\": item_id,\n",
    "            \"itemCategory\": item_cate,\n",
    "            \"historyItemIds\": item_history_sequence,\n",
    "            \"historyCategories\": cate_history_sequence,\n",
    "            \"currentTime\": current_time,\n",
    "            \"timeDiff\": time_diff,\n",
    "            \"timeFromFirstAction\": time_from_first_action,\n",
    "            \"timeToNow\": time_to_now\n",
    "        }\n",
    "\n",
    "    def load_dict(self, filename):\n",
    "        import pickle as pkl\n",
    "        \n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pkl.load(f)\n",
    "\n",
    "signature = infer_signature(model_input = {\n",
    "    \"userId\": \"A3R27T4HADWFFJ\",\n",
    "    \"itemIds\": [\"A\", \"B\", \"C\"],\n",
    "    \"timestamps\": [\"A\", \"B\", \"C\"]\n",
    "}, model_output = {\n",
    "    \"items\": [\"A\", \"B\"],\n",
    "    \"preds\": [3.14, 5.43]\n",
    "})\n",
    "\n",
    "default_conda_env = mlflow.pyfunc.get_default_conda_env()\n",
    "default_conda_env['dependencies'].append('tensorflow=2.12.0')\n",
    "default_conda_env['dependencies'].append('recommenders=1.2.0')\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"slirec_model\",\n",
    "        python_model=SliRecModelWrapper(),\n",
    "        conda_env=default_conda_env,\n",
    "        artifacts=artifacts,\n",
    "        registered_model_name=\"slirec_model\",\n",
    "        signature=signature\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c801b4-6f0c-43fd-9981-fc8b025c0e3d",
   "metadata": {},
   "source": [
    "## Additional steps\n",
    "\n",
    "### Restore trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f0c5f-596f-4ce7-b617-79ac751c11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = SequentialIterator\n",
    "yaml_serve_config_file = 'serving/model_serve_config.yaml'\n",
    "\n",
    "hparams = prepare_hparams(yaml_serve_config_file, \n",
    "                          user_vocab=user_vocab,\n",
    "                          item_vocab=item_vocab,\n",
    "                          cate_vocab=cate_vocab)\n",
    "\n",
    "trained_model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "trained_model.load_model(\"training/resources/training/model/best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229d61e-eb73-4ce3-acda-dbd42b4d74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.run_eval(test_file, num_ngs=test_num_ngs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f81aa6-bd43-4b2c-9e26-f02bb4898448",
   "metadata": {},
   "source": [
    "### Explore model details\n",
    "\n",
    "#### Inspect model input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fbf2066-51a4-42b7-979b-456a12f8ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "def load_dict(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        f_pkl = pkl.load(f)\n",
    "        return f_pkl\n",
    "\n",
    "def get_n(d, n):\n",
    "    return list(d.items())[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1948f03b-d74f-4481-a750-40d6e71844fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User dict len: 3487\n",
      "User embedding dimension: 16\n",
      "User embedding shape: [3487, 16]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ANCOMAI0I7LVG', 0),\n",
       " ('ABO2ZI2Y5DQ9T', 1),\n",
       " ('AQP1VPK16SVWM', 2),\n",
       " ('A19ZXK9HHVRV1X', 3),\n",
       " ('A2NJO6YE954DBH', 4),\n",
       " ('A16CZRQL23NOIW', 5),\n",
       " ('AWG2O9C42XW5G', 6),\n",
       " ('A3LZGLA88K0LA0', 7),\n",
       " ('A328S9RN3U5M68', 8),\n",
       " ('A1ER6IYOMM8VCT', 9)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_dict = load_dict(user_vocab)\n",
    "print(f\"User dict len: {len(user_dict)}\")\n",
    "print(f\"User embedding dimension: {hparams.user_embedding_dim}\")\n",
    "print(f\"User embedding shape: [{len(user_dict)}, {hparams.user_embedding_dim}]\")\n",
    "get_n(user_dict, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9609bfd2-98ff-468c-96f3-4e3aae7cb163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item dict len: 475\n",
      "Item embedding dimension: 32\n",
      "Item embedding shape: [475, 32]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('default_mid', 0),\n",
       " ('B0002KVUKM', 1),\n",
       " ('078886047X', 2),\n",
       " ('B00005JM0B', 3),\n",
       " ('B00005JLXH', 4),\n",
       " ('B00005JPY0', 5),\n",
       " ('B00003CXXO', 6),\n",
       " ('B0002Z0EXQ', 7),\n",
       " ('B0006SSOHC', 8),\n",
       " ('B00005JPS8', 9)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_dict = load_dict(item_vocab)\n",
    "print(f\"Item dict len: {len(item_dict)}\")\n",
    "print(f\"Item embedding dimension: {hparams.item_embedding_dim}\")\n",
    "print(f\"Item embedding shape: [{len(item_dict)}, {hparams.item_embedding_dim}]\")\n",
    "get_n(item_dict, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5390cc3-2273-4f7e-9c16-50967f42ea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cate dict len: 14\n",
      "Cate embedding dimension: 8\n",
      "Cate embedding shape: [14, 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('default_cat', 0),\n",
       " ('Movies', 1),\n",
       " ('TV', 2),\n",
       " ('Psychedelic Rock', 3),\n",
       " ('Album-Oriented Rock (AOR)', 4),\n",
       " ('Progressive Rock', 5),\n",
       " ('Alternative Rock', 6),\n",
       " ('Movies & TV', 7),\n",
       " ('Swing Jazz', 8),\n",
       " ('British Invasion', 9)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_dict = load_dict(cate_vocab)\n",
    "print(f\"Cate dict len: {len(cate_dict)}\")\n",
    "print(f\"Cate embedding dimension: {hparams.cate_embedding_dim}\")\n",
    "print(f\"Cate embedding shape: [{len(cate_dict)}, {hparams.cate_embedding_dim}]\")\n",
    "get_n(cate_dict, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f91778-cc3d-4daf-8657-cd0248f654ba",
   "metadata": {},
   "source": [
    "#### Inspect model placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb1a7428-efd8-4837-9c16-0b2d8bdc2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sequence_length, Outputs: [<tf.Tensor 'sequence_length:0' shape=(None,) dtype=int32>]\n",
      "Name: label, Outputs: [<tf.Tensor 'label:0' shape=(None, 1) dtype=float32>]\n",
      "Name: users, Outputs: [<tf.Tensor 'users:0' shape=(None,) dtype=int32>]\n",
      "Name: items, Outputs: [<tf.Tensor 'items:0' shape=(None,) dtype=int32>]\n",
      "Name: cates, Outputs: [<tf.Tensor 'cates:0' shape=(None,) dtype=int32>]\n",
      "Name: item_history, Outputs: [<tf.Tensor 'item_history:0' shape=(None, 50) dtype=int32>]\n",
      "Name: item_cate_history, Outputs: [<tf.Tensor 'item_cate_history:0' shape=(None, 50) dtype=int32>]\n",
      "Name: mask, Outputs: [<tf.Tensor 'mask:0' shape=(None, 50) dtype=int32>]\n",
      "Name: time, Outputs: [<tf.Tensor 'time:0' shape=(None,) dtype=float32>]\n",
      "Name: time_diff, Outputs: [<tf.Tensor 'time_diff:0' shape=(None, 50) dtype=float32>]\n",
      "Name: time_from_first_action, Outputs: [<tf.Tensor 'time_from_first_action:0' shape=(None, 50) dtype=float32>]\n",
      "Name: time_to_now, Outputs: [<tf.Tensor 'time_to_now:0' shape=(None, 50) dtype=float32>]\n",
      "Name: layer_keeps, Outputs: [<tf.Tensor 'layer_keeps:0' shape=<unknown> dtype=float32>]\n",
      "Name: is_training, Outputs: [<tf.Tensor 'is_training:0' shape=() dtype=bool>]\n",
      "Name: group, Outputs: [<tf.Tensor 'group:0' shape=() dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "model_placeholders = []\n",
    "for op in model.graph.get_operations():\n",
    "    if op.type == 'Placeholder':\n",
    "        model_placeholders.append(op)\n",
    "\n",
    "for p in model_placeholders:\n",
    "    print(f\"Name: {p.name}, Outputs: {p.outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44533eb9-848c-48b4-82a3-66b9c143416a",
   "metadata": {},
   "source": [
    "#### Inspect model via Tensorboard\n",
    "\n",
    "To inspect the model and its training process, make sure the info.write_tfevents is set to True in model_train_config.yaml config file. The summaries will be saved during model training inside the hparams.SUMMARIES_DIR directory.\n",
    "\n",
    "Run the TensorBoard via: $ tensorboard --logdir=<hparams.SUMMARIES_DIR>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc666fa8-1973-4ddc-b235-0e8fbd586221",
   "metadata": {},
   "source": [
    "#### Infer a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "928a363f-5f7f-420a-a137-26c588473b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 883, 0, 1, [10], [1], 1389657600.0, array([3.09104245]), array([3.09104245]), array([3.09104245]))\n",
      "{<tf.Tensor 'items:0' shape=(None,) dtype=int32>: array([0], dtype=int32), <tf.Tensor 'cates:0' shape=(None,) dtype=int32>: array([1], dtype=int32), <tf.Tensor 'item_history:0' shape=(None, 50) dtype=int32>: array([[10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0]], dtype=int32), <tf.Tensor 'item_cate_history:0' shape=(None, 50) dtype=int32>: array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]], dtype=int32), <tf.Tensor 'mask:0' shape=(None, 50) dtype=int32>: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.]], dtype=float32), <tf.Tensor 'time_from_first_action:0' shape=(None, 50) dtype=float32>: array([[3.0910425, 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       ]], dtype=float32), <tf.Tensor 'time_to_now:0' shape=(None, 50) dtype=float32>: array([[3.0910425, 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
      "        0.       , 0.       ]], dtype=float32)}\n",
      "[array([[0.6865755]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "it = model.iterator\n",
    "example_line1 = it.parser_one_line(\"0\\tA1U360OMVQRPUB\\tB0000AZT3R\\tMovies\\t1389657600\\tB000J10EQU\\tMovies\\t1387756800\")\n",
    "print(example_line1)\n",
    "\n",
    "(elabel1, euser_id1, eitem_id1, eitem_cate1, eitem_history_sequence1, eitem_cate_history_sequence1, ecurrent_time1,\n",
    "    etime_diff1, etime_from_first_action1, etime_to_now1) = example_line1\n",
    "\n",
    "data_dict = it._convert_data(\n",
    "    [elabel1],\n",
    "    [euser_id1],\n",
    "    [eitem_id1],\n",
    "    [eitem_cate1],\n",
    "    [eitem_history_sequence1],\n",
    "    [eitem_cate_history_sequence1],\n",
    "    [ecurrent_time1],\n",
    "    [etime_diff1],\n",
    "    [etime_from_first_action1],\n",
    "    [etime_to_now1],\n",
    "    0\n",
    ")\n",
    "\n",
    "feed_dict = {\n",
    "    it.items: data_dict[\"items\"],\n",
    "    it.cates: data_dict[\"cates\"],\n",
    "    it.item_history: data_dict[\"item_history\"],\n",
    "    it.item_cate_history: data_dict[\"item_cate_history\"],\n",
    "    it.mask: data_dict[\"mask\"],\n",
    "    it.time_from_first_action: data_dict[\"time_from_first_action\"],\n",
    "    it.time_to_now: data_dict[\"time_to_now\"],\n",
    "}\n",
    "print(feed_dict)\n",
    "\n",
    "pred = model.infer(model.sess, feed_dict)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
